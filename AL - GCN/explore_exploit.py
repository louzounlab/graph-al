import operator
import os
import numpy as np
from collections import Counter
import pyprind
import sys
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score
from random import shuffle
from scipy.stats import entropy
import torch
from sklearn.preprocessing import normalize
import math
import pandas as pd
from networkx.algorithms.shortest_paths import weighted
import networkx as nx


from graph_measures import feature_meta
from graph_measures.features_infra.feature_calculators import FeatureMeta
from graph_measures.features_infra.graph_features import GraphFeatures
from graph_measures.features_algorithms.vertices.page_rank import PageRankCalculator
from graph_measures.features_algorithms.vertices.attractor_basin import AttractorBasinCalculator
from gcn.data__loader import GraphLoader
from gcn.train import build_model, ModelRunner, mean_results, last_layer_representation
from tools import DistanceCalculator, DeepLearning, machine_learning, xgb_learning, aggregate, GraphNeighbors, \
    StandardML, k_truss

GPU_Device = "cuda:1"

# chose features for the model
CHOSEN_FEATURES = feature_meta.NODE_FEATURES
NEIGHBOR_FEATURES = feature_meta.NEIGHBOR_FEATURES



# available options
Available_Options = {'entropy', 'region_entropy', 'rep_dist', 'centrality', 'Chang', 'geo_dist', 'geo_cent',
                     'APR', 'k_truss', 'feature', 'random'}

# sub parameters are:
# entropy + region entropy: margin=True\False
# region entropy: region_average_entropy=True\False, region_include_self=True\False, region_only_out=True\False,
#                 region_weights='in'\'out'\'both', region_opposite_weights=True\False, region_prefer_large=True\False
#                 region_second_order=True\False
# representation distance: representation_vectors='model'\'topology'\'both', representation_motif4=True\False
#                          representation_measure='mahalanobis'\'euclidean'\'one_class_svm'\'isolation_forest'\
#                                                 'local_outlier_factor'\'robust_covariance'
#                          representation_average=True\False, representation_region=True\False
# geodesic distance: geo_in_out='both'\'in'\'out'
# relative page rank: apr_only_known=True\False
# feaure: feature='attractor_basin'


class GraphExploreExploit:

    def __init__(self, data_set, budget: int, opt='entropy'):

        self._model_runner = build_model(data_set)
        self._gl = self._model_runner.loader
        self._graph = self._gl.get_graph()
        self._nodes_order = np.array(self._gl.nodes_order)
        self._models = {}
        self._opt = opt
        self.eval_method = self._opt
        self._labels = self._gl.labels.cpu().numpy()
        self._labels_list = self._gl.labels_list
        self._n_classes = self._gl.num_labels
        self._num_nodes = self._gl.data_size
        self._pool = np.array([])
        self._revealed = np.array([])
        self._num_revealed = 0
        self._device = torch.device(GPU_Device if torch.cuda.is_available() else "cpu")
        if budget < 1:
            self._stop_cond = math.ceil(budget * self._num_nodes)
        else:
            self._stop_cond = budget
        print("budget is {0:.2f}% of the data".format(self._stop_cond/self._num_nodes*100))

    def _init(self, n_nodes=1):
        # set train size to 0
        self._gl.split_test(1, build_features=True)
        self._pool = self._gl.test_idx.cpu().numpy()
        self._revealed = np.array([])
        self._num_revealed = 0

        # evaluate the model without labeled nodes
        results = self._model_runner.test()

        # randomly chose n_nodes nodes from each label
        c = {x: 0 for x in self._labels_list}
        indices = self._gl.test_idx
        indices = indices[torch.randperm(len(indices))].cpu().numpy()
        i = 0
        while min(c.values()) < n_nodes:
            tag = self._gl.get_tag_i(indices[i])
            if c[tag] < n_nodes:
                self._gl.idx_test_to_train(indices[i], build_features=False)
                c[tag] += 1
            i += 1
        self._pool = self._gl.test_idx.cpu().numpy()
        self._revealed = self._gl.base_train_idx.cpu().numpy()
        self._gl.build_features_matrix()

        return results

    def _create_method_name(self, eps, **params):
        method_name = self._opt

        if params.get('balance', False):
            method_name += '_BAL'

        if params.get('margin', False):
            method_name = method_name.replace('entropy', 'margin')

        if params.get('region_average_entropy', False):
            method_name += '_AE'
        if params.get('region_include_self', False):
            method_name += '_S'
        if params.get('region_only_out', False):
            method_name += '_OO'
        if params.get('region_weights', None):
            if params['region_weights'] == 'in':
                method_name += '_WI'
            elif params['region_weights'] == 'out':
                method_name += '_WO'
            else:
                method_name += '_W'
        if params.get('region_opposite_weights', False):
            method_name += '_OW'
        if params.get('region_prefer_large', False):
            method_name += '_PL'
        if params.get('region_second_order', False):
            method_name += '_SO'

        if params.get('representation_region', False):
            method_name += '_REG'
        if params.get('representation_vectors', None):
            if params['representation_vectors'] == 'both':
                method_name += '_B'
            elif params['representation_vectors'] == 'topology':
                method_name += '_T'
            else:
                method_name += '_M'
        if params.get('representation_motif4', False):
            method_name += '_M4'
        if params.get('representation_measure', None):
            if params['representation_measure'] == 'euclidean':
                method_name += '_EUC'
            elif params['representation_measure'] == 'one_class_svm':
                method_name += '_SVM'
            elif params['representation_measure'] == 'mahalanobis':
                method_name += '_MAH'
            elif params['representation_measure'] == 'isolation_forest':
                method_name += '_ISF'
            elif params['representation_measure'] == 'local_outlier_factor':
                method_name += '_LOF'
            elif params['representation_measure'] == 'robust_covariance':
                method_name += '_RC'
        if params.get('representation_average', False):
            method_name += '_AVG'

        if params.get('geo_in_out', None):
            if params['geo_in_out'] == 'in':
                method_name += '_I'
            elif params['geo_in_out'] == 'out':
                method_name += '_O'
            else:
                method_name += '_B'

        if params.get('apr_only_known', False):
            method_name += '_OK'

        if params.get('feature', None):
            if params['feature'] == 'attractor_basin':
                method_name += '_AB'

        self.eval_method = method_name + " {}".format(eps)
        return

    def _reveal(self, idx, build_features=True):
        # self._gl.split_train(1, build_features=False)
        for i in idx:
            # self._gl.idx_test_to_val(self._pool[i], build_features=False)
            self._gl.idx_test_to_train(self._pool[i], build_features=False)
            self._num_revealed += 1
        self._revealed = np.append(self._revealed, self._pool[list(idx)])
        self._pool = np.delete(self._pool, list(idx), 0)
        if build_features:
            self._gl.build_features_matrix()

    def _train_and_eval(self, epochs, iterations=1, clear_model=True, verbose=1):
        res = []
        for j in range(iterations):
            if clear_model:
                self._models.clear()
            self._models = self._model_runner.train(epochs, self._models, verbose=verbose, early_stopping=False)
            res.append(self._model_runner.test(self._models))
        mean_res = mean_results(res)

        return mean_res

    def _explore(self, batch_size, balance=False):
        if not balance:
            idx = np.random.choice(len(self._pool), batch_size, replace=False)
        else:
            # balancing classes
            idx = set()
            classes_revealed = Counter(self._labels[self._revealed])
            for _ in range(batch_size):
                desired_class = min(classes_revealed.items(), key=operator.itemgetter(1))[0]
                classes_revealed[desired_class] += 1
                pool = np.where(self._labels == desired_class)
                idx.add(np.random.choice(np.intersect1d(pool, self._pool)))

        # # try balancing classes
        # idx = set()
        # test_probs = self._model_runner.predict_proba(self._models).cpu().numpy()
        # classes_revealed = Counter(self._labels[self._revealed])
        # # for _ in range(batch_size):
        # while len(idx) < batch_size:
        #     desired_class = min(classes_revealed.items(), key=operator.itemgetter(1))[0]
        #     classes_revealed[desired_class] += 1
        #     class_most_likely = np.argmax(test_probs[:, desired_class])
        #     idx.add(class_most_likely)
        #     test_probs[class_most_likely] = np.zeros(self._n_classes)

        return idx

    def _representation_distance(self, vectors_type='model', dist_measure='mahalanobis', features_matrix=None,
                                 inv_cov=None, average_distance=False, region=False, neighbors_matrix=None):
        if vectors_type == 'topology':
            rep_vectors = features_matrix
        if vectors_type in {'model', 'both'}:
            rep = last_layer_representation(self._models)
            for key in rep.keys():
                rep_vectors = rep[key].cpu().detach().numpy()
            if vectors_type == 'both':
                rep_vectors = np.c_[rep_vectors, features_matrix]

        if region:
            new_rep = []
            for node in range(self._num_nodes):
                reg_matrix = neighbors_matrix[node]
                if len(reg_matrix) == 0:
                    new_rep.append(np.zeros(rep_vectors[node].shape))
                else:
                    reg_rep = rep_vectors[reg_matrix]
                    new_rep.append(np.average(reg_rep, axis=0))
            rep_vectors = np.array(new_rep)

        pool_rep = rep_vectors[self._pool]
        train_rep = rep_vectors[self._gl.base_train_idx.cpu()]

        if dist_measure in {'one_class_svm', 'isolation_forest', 'local_outlier_factor', 'robust_covariance'}:
            outlier_score = DistanceCalculator.one_class_learning(train_rep, pool_rep, typ=dist_measure, scores=True)
        else:
            outlier_score = DistanceCalculator.metric(pool_rep, train_rep, typ=dist_measure, inv_cov=inv_cov,
                                                      average_distance=average_distance, scores=True)
        return outlier_score

    def _region_entropies(self, neighbors_matrix, weights=None, prefer_large_regions=False, average_entropy=False,
                          margin=False):
        classes_probs = self._model_runner.predict_proba(self._models, test_only=False).cpu().numpy()
        region_entropies = []
        for node in self._pool:
            reg_matrix = neighbors_matrix[node]
            if len(reg_matrix) == 0:
                region_entropies.append(0)
            else:
                reg_proba = classes_probs[reg_matrix]
                if weights is None:
                    w = weights
                else:
                    w = weights[reg_matrix]

                if margin:
                    if average_entropy:
                        reg_proba.sort()
                        reg_ent = np.average(reg_proba[:, -1]-reg_proba[:, -2], weights=w)
                    else:
                        avg_prob = np.average(reg_proba, axis=0, weights=w)
                        avg_prob.sort()
                        reg_ent = abs(avg_prob[-1]-avg_prob[-2])
                else:
                    if average_entropy:
                        reg_ent = np.average(entropy(reg_proba.T), weights=w)
                    else:
                        reg_ent = entropy(np.average(reg_proba, axis=0, weights=w))

                if prefer_large_regions:
                    reg_ent *= len(reg_matrix)/(len(reg_matrix)+1)

                region_entropies.append(reg_ent)

        return region_entropies

    def _geodesic_distance(self, graph_dists, ancestors, descendants, in_out='both'):
        dists = []
        nodes_revealed = self._nodes_order[self._revealed]
        for i in self._pool:
            node = self._nodes_order[i]
            dist = []
            if in_out is not 'out':
                anc = ancestors[i].intersection(nodes_revealed)
                dist.extend([graph_dists[x][node] for x in anc])
            if in_out is not 'in':
                des = descendants[i].intersection(nodes_revealed)
                dist.extend([graph_dists[node][x] for x in des])

            if dist:
                dists.append(min(dist))
            else:
                dists.append(9)
        dist_to_train = np.array(dists)

        return dist_to_train

    def _relative_page_rank(self, page_rank, a_tilda, gamma=0.9, only_known=False):
        a_tilda = a_tilda[self._pool] * gamma
        a1 = a_tilda[:, self._revealed]
        a1 = torch.matmul(a1, page_rank[self._revealed])
        a1 = a1 + ((1-gamma)/self._num_nodes)
        a2 = a_tilda[:, self._pool]
        a2 = torch.eye(len(self._pool), device=self._device) - a2
        try:
            a2 = torch.inverse(a2)
        except:
            a2 = torch.pinverse(a2)
        known_rank = torch.matmul(a2, a1)

        if only_known:
            apr = known_rank
        else:
            apr = (page_rank[self._pool] / known_rank)

        return apr.cpu().numpy()

    def _active_learning(self, eps=0.05, batch_size=5, epochs=200, out_prog=True, clear_model=False, out_interval=25,
                         **params):

        if self._opt not in Available_Options:
            print("option {} is not supported".format(self._opt))
            return

        # preliminary calculation for exploitation
        neighbors_matrix = None
        if self._opt in {'region_entropy'} or params.get('representation_region', False):
            gn = GraphNeighbors(self._graph)
            neighbors_matrix = gn.neighbors(second_order=params.get('region_second_order', False),
                                            self_node=params.get('region_include_self', False),
                                            with_orders=False, only_out=params.get('region_only_out', False))

            if params.get('region_weights', None):
                if params['region_weights'] == 'out':
                    neighbors_degree = np.array(self._graph.out_degree(self._nodes_order))[:, 1]
                elif params['region_weights'] == 'in':
                    neighbors_degree = np.array(self._graph.in_degree(self._nodes_order))[:, 1]
                else:
                    neighbors_degree = np.array(self._graph.degree(self._nodes_order))[:, 1]

                neighbors_degree = neighbors_degree.astype(float)
                for x in neighbors_matrix:
                    for j in range(len(x) - 1, -1, -1):
                        if neighbors_degree[x[j]] == 0:
                            del x[j]
                if params.get('region_opposite_weights', False):
                    weights = neighbors_degree
                else:
                    weights = 1 / neighbors_degree
                    weights[weights == np.inf] = 0
            else:
                weights = None

        if self._opt in {'centrality', 'Chang', 'geo_cent', 'APR'}:
            page_rank_feature = {"page_rank": FeatureMeta(PageRankCalculator, {"pr"})}
            page_rank_matrix = self._gl.get_features(page_rank_feature)
            norm_page_rank = normalize(page_rank_matrix, axis=0, norm='max').reshape(-1)
            if self._opt == 'APR':
                adj_matrix = nx.adjacency_matrix(self._graph, nodelist=self._nodes_order).todense()
                np.fill_diagonal(adj_matrix, 0)
                adj_matrix = adj_matrix / adj_matrix.sum(axis=1)
                adj_matrix = np.nan_to_num(adj_matrix)
                adj_matrix = adj_matrix.T
                if torch.cuda.is_available():
                    page_rank_matrix = torch.Tensor(page_rank_matrix).to(self._device)
                    adj_matrix = torch.Tensor(adj_matrix).to(self._device)

        if self._opt in {'rep_dist', 'Chang'}:
            inv_cov = None
            features_matrix = None
            if params.get('representation_vectors', 'model') in {'topology', 'both'}:
                rep_features = CHOSEN_FEATURES.copy()
                if not params.get('representation_motif4', False):
                    rep_features.pop('motif4')
                features_matrix = self._gl.get_features(rep_features, print_time=True)
                # replace all nan values of attractor basin to 100
                features_matrix[np.isnan(features_matrix)] = 100
                if params.get('representation_measure', 'mahalanobis') == 'mahalanobis' and \
                        params['representation_vectors'] == 'topology':
                    inv_cov = np.linalg.pinv(np.cov(features_matrix.T))

        if self._opt in {'geo_dist', 'geo_cent'}:
            graph_dists = dict(weighted.all_pairs_dijkstra_path_length(self._graph, self._num_nodes, weight='weight'))
            ancestors = [nx.ancestors(self._graph, node) for node in sorted(self._graph)]
            descendants = [nx.descendants(self._graph, node) for node in sorted(self._graph)]

        if self._opt == 'k_truss':
            k_truss_score = k_truss(self._graph)

        if self._opt == 'feature':
            if params.get('feature', 'attractor_basin') == 'attractor_basin':
                chosen_feature = {"attractor_basin": FeatureMeta(AttractorBasinCalculator, {"ab"})}
            feature_score = self._gl.get_features(chosen_feature)

        # initializing parameters for plots
        if out_prog:
            results = {0: self._init(n_nodes=1)}
            prog_intervals = out_interval
            prog_iter = 1
            cur_interval = 1
        else:
            self._init()
            results = {}

        while self._num_revealed < self._stop_cond:
            # making output for plot
            if out_prog and self._num_revealed >= cur_interval/prog_intervals*self._stop_cond:
                res = self._train_and_eval(epochs, iterations=prog_iter, clear_model=clear_model)
                percent_revealed = round(self._num_revealed / self._num_nodes * 100, 2)
                results[percent_revealed] = res
                val = list(res.values())[0]
                self._model_runner.data_logger.info(self.eval_method, percent_revealed,
                                                    val["loss"], val["acc"], val["mic_f1"], val["mac_f1"])
                cur_interval = np.floor(self._num_revealed/self._stop_cond*prog_intervals) + 1
            elif self._opt in {'entropy', 'region_entropy', 'rep_dist', 'Chang'}:
                self._train_and_eval(epochs, clear_model=clear_model, verbose=0)

            # eps greedy algorithm choosing nodes to reveal
            rand = np.random.uniform(0, 1)
            # Explore
            if rand < eps or self._opt == 'random':
                idx = self._explore(batch_size=batch_size, balance=params.get('balance', False))
            # Exploit
            else:
                # evaluate the samples
                if self._opt in {'rep_dist', 'Chang'}:
                    outlier_score = self._representation_distance(vectors_type=params.get('representation_vectors',
                                                                                          'model'),
                                                                  dist_measure=params.get('representation_measure',
                                                                                          'mahalanobis'),
                                                                  features_matrix=features_matrix, inv_cov=inv_cov,
                                                                  average_distance=params.get('representation_average',
                                                                                              False),
                                                                  region=params.get('representation_region', False),
                                                                  neighbors_matrix=neighbors_matrix)

                if self._opt in {'entropy', 'Chang'}:
                    test_probs = self._model_runner.predict_proba(self._models).cpu()
                    if params.get('margin', False):
                        test_probs.sort()
                        entropies = test_probs[:, -1] - test_probs[:, -2]
                    else:
                        entropies = entropy(test_probs.t())

                if self._opt in {'region_entropy'}:
                    region_entropies = self._region_entropies(neighbors_matrix=neighbors_matrix, weights=weights,
                                                              average_entropy=params.get('region_average_entropy',
                                                                                         False),
                                                              prefer_large_regions=params.get('region_prefer_large',
                                                                                              False),
                                                              margin=params.get('margin', False))

                if self._opt in {'geo_dist', 'geo_cent'}:
                    dist_to_train = self._geodesic_distance(graph_dists, ancestors, descendants,
                                                            in_out=params.get('geo_in_out', 'both'))

                if self._opt == 'APR':
                    relative_pr = self._relative_page_rank(page_rank_matrix, adj_matrix,
                                                           only_known=params.get('apr_only_known', False))

                # build scores vector according to the evaluation method
                if self._opt == 'entropy':
                    scores = entropies
                elif self._opt == 'region_entropy':
                    scores = region_entropies
                elif self._opt == 'rep_dist':
                    scores = outlier_score
                elif self._opt == 'geo_dist':
                    scores = dist_to_train
                elif self._opt == 'centrality':
                    scores = np.asarray([page_rank_matrix[x] for x in self._pool]).reshape(-1)
                elif self._opt == 'k_truss':
                    scores = np.asarray([k_truss_score[x] for x in self._pool]).reshape(-1)
                elif self._opt == 'feature':
                    scores = np.asarray([feature_score[x] for x in self._pool]).reshape(-1)
                elif self._opt == 'APR':
                    scores = relative_pr.flat
                elif self._opt == 'Chang':
                    c3 = max(0.7 * (1 - self._num_revealed/self._stop_cond), 0)
                    c1 = c2 = (1 - c3) / 2

                    density = normalize(np.nan_to_num(outlier_score).reshape(1, -1)).reshape(-1)
                    norm_entropy = normalize(entropies.reshape(1, -1)).reshape(-1)
                    centrality = np.asarray([norm_page_rank[x] for x in self._pool]).reshape(-1)

                    scores = c1 * norm_entropy + c2 * density + c3 * centrality
                elif self._opt == 'geo_cent':
                    c1 = 0.7
                    c2 = 0.3

                    g_dists = normalize(dist_to_train.reshape(1, -1), norm='max').reshape(-1)
                    centrality = np.vstack(norm_page_rank[x] for x in self._pool).reshape(-1)

                    scores = c1 * g_dists + c2 * centrality

                if params.get('margin', False):
                    # choose the lowest scores nodes
                    idx = np.argpartition(scores, batch_size)[:batch_size]
                else:
                    # choose the best nodes
                    idx = np.argpartition(scores, -batch_size)[-batch_size:]

            self._reveal(idx)

        # evaluate the model
        res = self._train_and_eval(epochs, clear_model=clear_model)
        percent_revealed = round(self._num_revealed / self._num_nodes * 100, 2)
        results[percent_revealed] = res
        val = list(res.values())[0]
        self._model_runner.data_logger.info(self.eval_method, percent_revealed,
                                            val["loss"], val["acc"], val["mic_f1"], val["mac_f1"])

        return results

    def run(self, iterations=3, eps=0.05, batch_size=5, epochs=200, clear_model=True, out_prog=True, out_interval=25,
            option=None, **option_parameters):

        if option:
            self._opt = option
        self._create_method_name(eps, **option_parameters)
        if batch_size < 1:
            batch_size = min(max(int(batch_size * self._num_nodes), 3), 20)

        res = [self._active_learning(eps, batch_size, epochs, out_prog, clear_model, out_interval, **option_parameters)
               for _ in range(iterations)]
        aggregated = aggregate(res)
        result = {}
        for name, res in aggregated.items():
            result[name] = {}
            # print("std of method {} is:".format(self.eval_method))
            for train_s, vals in res.items():
                result[name][train_s] = {}
                val_list = sorted(vals.items(), key=lambda x: x[0], reverse=True)
                result[name][train_s] = {key: np.mean(val) for key, val in val_list}
                # print(train_s)
                # print({key: np.std(val) for key, val in val_list})

        return result

