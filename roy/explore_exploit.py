from scipy.spatial.distance import cdist
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from collections import Counter
from datetime import datetime
import pyprind
import sys
import time

from sklearn.model_selection import train_test_split


class DistanceCalculator:
    # def __init__(self, mx, typ="euclidean"):
    #     self._mx = mx
    #     self._type = typ

    @staticmethod
    def euclidean(mx1, mx2, typ="euclidean"):
        # Get euclidean distances as 2D array
        dists = cdist(mx1, mx2, typ)
        # return the most distant rows
        return np.unravel_index(dists.argmax(), (mx1.shape[0], mx2.shape[0]))

    # TODO:implement using one class svm- http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html
    @staticmethod
    def one_class_learning(x_train, x_test):
        model = svm.OneClassSVM(gamma='scale')
        model.fit(x_train)
        pred = model.predict(x_test)
        indices = np.where(pred == -1)[0]
        if len(indices) >= 1:
            random_idx = np.random.randint(0, len(indices))
            return indices[random_idx]
        else:
            return np.random.randint(0, len(x_test))


def machine_learning(x_train, y_train, x_test, batch_size=10, clf=None, is_stand_ml=False, is_al=False):
    if clf is None:
        clf = RandomForestClassifier(n_estimators=200, class_weight="balanced")
    clf.fit(np.asmatrix(x_train, dtype=np.float32), y_train)
    probs = clf.predict_proba(x_test)
    if is_stand_ml:
        return probs
    if is_al:
        certainty = [abs(a-b) for (a, b) in probs]
        return np.argpartition(certainty, -batch_size)[-batch_size:]
    # returns the batch_size highest probability indexes
    return np.argpartition(probs[:, 1], -batch_size)[-batch_size:]


class DeepLearning:
    def __init__(self, x_shape):
        from keras import Sequential
        from keras.callbacks import EarlyStopping
        from keras.layers import Dense, Dropout
        from keras.regularizers import l1_l2
        self.early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=50, mode='min', verbose=1, )
        self.classifier = Sequential()
        self.classifier.add(Dense(300, kernel_initializer="he_normal", activation="elu", input_dim=x_shape))
        self.classifier.add(Dropout(0.3))
        self.classifier.add(Dense(450, kernel_initializer='he_normal', activation='elu'))
        self.classifier.add(Dropout(0.3))
        self.classifier.add(Dense(100, kernel_initializer='he_normal', activation='elu'))
        self.classifier.add(Dropout(0.3))
        self.classifier.add(Dense(20, kernel_initializer='he_normal', activation='elu', kernel_regularizer=l1_l2()))
        self.classifier.add(Dense(1, kernel_initializer='uniform', activation="sigmoid",
                                  activity_regularizer=l1_l2(0.005, 0.005)))

        self.classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # TODO: implementation using keras
    def learn(self, x_train, y_train, x_test, batch_size=10, is_stand_ml=False, is_al=False):
        self.classifier.fit(x_train, y_train, validation_split=0.1, callbacks=[self.early_stopping], epochs=100,
                            batch_size=64, verbose=0)

        probs = self.classifier.predict_proba(x_test)

        if is_stand_ml:
            return probs
        if is_al:
            certainty = [abs(a-0.5) for a in probs]
            return np.argmin(certainty)
        return np.argpartition(probs[:, 1], -batch_size)[-batch_size:]


class ExploreExploit:
    def __init__(self, features_mx, tags_vect, recall, eps):
        self._tags = tags_vect
        self._features_mx = features_mx
        self._smallest_class = Counter(tags_vect).most_common()[-1][0]
        # self._largest_class = Counter(tags_vect).most_common()[0][0]
        self._n_black = Counter(tags_vect).most_common()[-1][1]
        self._tags1 = [1 if y == self._smallest_class else 0 for y in self._tags]
        self._stop_cond = np.round(recall * self._n_black)
        self._eps = eps
        self._recall = recall
        self._time = 0
        self._num_black_found = 0
        self._num_nodes = len(tags_vect)

    def _init(self):
        # initialize the train objects with copies
        self.x_test = type(self._features_mx)(self._features_mx)
        self.y_test = list(self._tags1)
        self.x_train = None
        self.y_train = None
        self.bar = pyprind.ProgBar(len(self._tags), stream=sys.stdout)
        # explore first using distance
        idx1, idx2 = DistanceCalculator.euclidean(self.x_test, self.x_test)
        self._reveal(idx1)
        self._reveal(idx2-1)    # we deleted idx1 from the test, therefore the original index idx2 is now idx2-1
                                # notice idx1 is smaller then idx2

    def run_recall(self, dist_calc_type, learn_type="ML", batch_size=10):
        self._num_black_found = 0
        self._init()
        time_per_recall = {round(x, 2): 0 for x in np.arange(0, self._recall + 0.05, 0.05)}
        i = 1

        if learn_type == "DL":
            net = DeepLearning(self.x_train.shape[1])

        while self._num_black_found < self._stop_cond:
            rand = np.random.uniform(0, 1)
            if rand < self._eps or len(Counter(self.y_train)) <= 1:
                if dist_calc_type == "euclidean":
                    idx = [DistanceCalculator.euclidean(self.x_test, self.x_train)[0]]
                else:
                    idx = [DistanceCalculator.one_class_learning(self.x_train, self.x_test)]
            else:
                if learn_type == "DL":
                    idx = net.learn(self.x_train, self.y_train, self.x_test, batch_size=batch_size)
                else:
                    idx = machine_learning(self.x_train, self.y_train, self.x_test, batch_size=batch_size)
            for j in idx:
                self._reveal(j)
                if self._num_black_found == np.round(self._n_black * 0.05 * i):
                    time_per_recall[round(i * 0.05, 2)] = self._time / self._num_nodes
                    i += 1

        return time_per_recall, self.y_train

    def _reveal(self, idx):
        self._time += 1
        self.bar.update()
        # if self._time % 100 == 0:
        #     print(str(self._time) + " nodes were explored, time:" + str(datetime.now().time()))
        if self.y_test[idx] == 1:
            self._num_black_found += 1
        if self.x_train is None:
            self.x_train = self.x_test[idx, :]
            self.y_train = []
        else:
            self.x_train = np.vstack([self.x_train, self.x_test[idx, :]])
        self.y_train.append(self.y_test.pop(idx))
        self.x_test = np.delete(self.x_test, idx, 0)

    def run_accuracy(self, dist_calc_type, learn_type="ML"):
        self._init()

        if learn_type == "DL":
            net = DeepLearning(self.x_train.shape[1])

        while self._time < self._num_nodes // 5:
            rand = np.random.uniform(0, 1)
            if rand < self._eps or len(Counter(self.y_train)) <= 1:
                if dist_calc_type == "euclidean":
                    idx = DistanceCalculator.euclidean(self.x_test, self.x_train)[0]
                else:
                    idx = DistanceCalculator.one_class_learning(self.x_train, self.x_test)
            else:
                if learn_type == "DL":
                    idx = net.learn(self.x_train, self.y_train, self.x_test, is_al=True)
                else:
                    idx = machine_learning(self.x_train, self.y_train, self.x_test, is_al=True)
            self._reveal(idx)

        # check standard ML results after revealing 20% of th data
        probs_ml = machine_learning(self.x_train, self.y_train, self.x_test, is_stan_ml=True)
        pred_ml = [1 if probs_ml[i, 1] >= 0.5 else 0 for i in range(len(probs_ml))]
        correct_ml = [1 if pred_ml[i] == self.y_test[i] else 0 for i in range(len(pred_ml))]
        accuracy_20 = sum(correct_ml) / len(correct_ml)
        print("mean acc using active learning: " + str(accuracy_20))

        return self.y_train, accuracy_20


class StandardML:
    def __init__(self, features_mx, tags_vect):
        self._tags = tags_vect
        self._features_mx = features_mx
        self._smallest_class = Counter(tags_vect).most_common()[-1][0]
        # self._largest_class = Counter(tags_vect).most_common()[0][0]
        self._n_black = Counter(tags_vect).most_common()[-1][1]
        self._tags1 = [1 if y == self._smallest_class else 0 for y in self._tags]
        self._time = 0
        self._num_nodes = len(tags_vect)

    def _init(self, train_size=0.2):
        # initialize the train objects with copies
        self.x_data = type(self._features_mx)(self._features_mx)
        self.y_data = list(self._tags1)
        # split to train and test
        indices = list(range(self._num_nodes))
        split = int(np.round(self._num_nodes * train_size))
        train_idx = np.random.choice(indices, size=split, replace=False)
        test_idx = list(set(indices) - set(train_idx))
        self.x_train = np.vstack([self.x_data[i] for i in train_idx])
        self.y_train = [self.y_data[i] for i in train_idx]
        self.x_test = np.vstack([self.x_data[i] for i in test_idx])
        self.y_test = [self.y_data[i] for i in test_idx]

    def run(self):
        self._init()

        probs_ml = machine_learning(self.x_train, self.y_train, self.x_test, is_stand_ml=True)
        pred_ml = [1 if probs_ml[i, 1] >= 0.5 else 0 for i in range(len(probs_ml))]
        correct_ml = [1 if pred_ml[i] == self.y_test[i] else 0 for i in range(len(pred_ml))]
        acc_ml = sum(correct_ml)/len(correct_ml)

        net = DeepLearning(self.x_train.shape[1])
        probs_dl = net.learn(self.x_train, self.y_train, self.x_test, is_stand_ml=True)
        pred_dl = [1 if probs_dl[i] >= 0.5 else 0 for i in range(len(probs_dl))]
        correct_dl = [1 if pred_dl[i] == self.y_test[i] else 0 for i in range(len(pred_dl))]
        acc_dl = sum(correct_dl) / len(correct_dl)

        return acc_ml, acc_dl

    def recall_per_data(self, train_size=0.2, certainty_rate=0.5):
        if train_size == 1:
            return 1, 1

        self._init(train_size)
        probs_ml = machine_learning(self.x_train, self.y_train, self.x_test, is_stand_ml=True)
        # revealing the true label of all nodes which classified as black
        tags = [self.y_test[i] for i in np.where(probs_ml[:, 1] >= certainty_rate)[0]]

        black_found = sum(self.y_train) + sum(tags)
        nodes_revealed = len(self.y_train) + len(tags)
        recall = black_found / self._n_black
        steps = nodes_revealed / self._num_nodes

        return recall, steps




