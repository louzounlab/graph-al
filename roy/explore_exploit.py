from scipy.spatial.distance import cdist
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from collections import Counter
import pyprind
import sys

# from sklearn.model_selection import train_test_split


class DistanceCalculator:
    # def __init__(self, mx, typ="euclidean"):
    #     self._mx = mx
    #     self._type = typ

    @staticmethod
    def euclidean(mx1, mx2, typ="euclidean"):
        # Get euclidean distances as 2D array
        dists = cdist(mx1, mx2, typ)
        # return the most distant rows
        return np.unravel_index(dists.argmax(), (mx1.shape[0], mx2.shape[0]))

    # TODO: check more outlier detection methods, https://scikit-learn.org/stable/modules/outlier_detection.html
    @staticmethod
    def one_class_learning(x_train, x_test):
        model = svm.OneClassSVM(gamma='scale')
        model.fit(x_train)
        pred = model.predict(x_test)
        indices = np.where(pred == -1)[0]
        if len(indices) >= 1:
            random_idx = np.random.randint(0, len(indices))
            return indices[random_idx]
        else:
            return np.random.randint(0, len(x_test))


def machine_learning(x_train, y_train, x_test, batch_size=10, clf=None, is_stand_ml=False, is_al=False):
    if clf is None:
        clf = RandomForestClassifier(n_estimators=200, class_weight="balanced")
    clf.fit(np.asmatrix(x_train, dtype=np.float32), y_train)
    probs = clf.predict_proba(x_test)
    if is_stand_ml:
        return probs
    if is_al:
        certainty = [abs(a-b) for (a, b) in probs]
        return np.argpartition(certainty, -batch_size)[-batch_size:]
    # returns the batch_size highest probability indexes
    return np.argpartition(probs[:, 1], -batch_size)[-batch_size:]



class DeepLearning:
    def __init__(self, x_shape):
        from keras import Sequential
        from keras.callbacks import EarlyStopping
        from keras.layers import Dense, Dropout
        from keras.regularizers import l1_l2
        self.early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=50, mode='min', verbose=1, )
        self.classifier = Sequential()
        self.classifier.add(Dense(300, kernel_initializer="he_normal", activation="elu", input_dim=x_shape))
        self.classifier.add(Dropout(0.3))
        self.classifier.add(Dense(450, kernel_initializer='he_normal', activation='elu'))
        self.classifier.add(Dropout(0.3))
        self.classifier.add(Dense(100, kernel_initializer='he_normal', activation='elu'))
        self.classifier.add(Dropout(0.3))
        self.classifier.add(Dense(20, kernel_initializer='he_normal', activation='elu', kernel_regularizer=l1_l2()))
        self.classifier.add(Dense(1, kernel_initializer='uniform', activation="sigmoid",
                                  activity_regularizer=l1_l2(0.005, 0.005)))

        self.classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # TODO: implementation using keras
    def learn(self, x_train, y_train, x_test, batch_size=10, is_stand_ml=False, is_al=False):
        self.classifier.fit(x_train, y_train, validation_split=0.1, callbacks=[self.early_stopping], epochs=100,
                            batch_size=64, verbose=0)
        probs = self.classifier.predict_proba(x_test)

        if is_stand_ml:
            return probs
        if is_al:
            certainty = [abs(a-0.5) for a in probs]
            return np.argpartition(certainty, -batch_size)[-batch_size:]
        return np.argpartition(probs[:, 1], -batch_size)[-batch_size:]


class ExploreExploit:
    def __init__(self, features_mx, tags_vect, label, recall):
        self._tags = tags_vect
        self._features_mx = features_mx
        self._smallest_class = Counter(tags_vect).most_common()[-1-label][0]
        # self._largest_class = Counter(tags_vect).most_common()[0][0]
        self._n_black = Counter(tags_vect).most_common()[-1-label][1]
        self._tags1 = [1 if y == self._smallest_class else 0 for y in self._tags]
        self._stop_cond = np.round(recall * self._n_black)
        self._recall = recall
        self._time = 0
        self._num_black_found = 0
        self._num_nodes = len(tags_vect)
        print("there are " + str(self._n_black) + " nodes to find, which are "
              + str(self._n_black/self._num_nodes*100) + "% of the whole data")

    def _init(self):
        self._time = 0
        self._num_black_found = 0
        self._bar = pyprind.ProgBar(len(self._tags), stream=sys.stdout)
        # initialize the train objects with copies
        self.x_test = type(self._features_mx)(self._features_mx)
        self.y_test = list(self._tags1)
        self.x_train = None
        self.y_train = None
        # explore first using distance
        idx1, idx2 = DistanceCalculator.euclidean(self.x_test, self.x_test)
        self._reveal(idx1)
        self._reveal(idx2-1)    # we deleted idx1 from the test, therefore the original index idx2 is now idx2-1
                                # notice idx1 is smaller then idx2

    def run_recall(self, dist_calc_type, learn_type="ML", batch_size=10, eps=0):
        self._init()

        time_per_recall = {round(x, 2): 0 for x in np.arange(0, self._recall + 0.01, 0.05)}
        i = 1

        if learn_type == "DL":
            net = DeepLearning(self.x_train.shape[1], len(Counter(self.y_train)))

        while self._num_black_found < self._stop_cond:
            rand = np.random.uniform(0, 1)
            if rand < eps or len(Counter(self.y_train)) <= 1:
                if dist_calc_type == "euclidean":
                    idx = [DistanceCalculator.euclidean(self.x_test, self.x_train)[0]]
                else:
                    idx = [DistanceCalculator.one_class_learning(self.x_train, self.x_test)]
            else:
                if learn_type == "DL":
                    idx = net.learn(self.x_train, self.y_train, self.x_test, batch_size=min(batch_size,
                                                                                            self._num_black_found))
                else:
                    idx = machine_learning(self.x_train, self.y_train, self.x_test,
                                           batch_size=min(batch_size, self._num_black_found))
            for j in sorted(idx, reverse=True):
                self._reveal(j)
                if self._num_black_found == np.round(self._n_black * 0.05 * i):
                    time_per_recall[round(i * 0.05, 2)] = self._time / self._num_nodes
                    temp = self._num_black_found
                    i += 1
                    while np.round(self._n_black * 0.05 * i) == temp:
                        time_per_recall[round(i * 0.05, 2)] = self._time / self._num_nodes
                        i += 1
        print(" ")
        return time_per_recall, self.y_train

    def _reveal(self, idx):
        self._time += 1
        self._bar.update()
        # if self._time % 100 == 0:
        #     print(str(self._time) + " nodes were explored, time:" + str(datetime.now().time()))
        if self.y_test[idx] == 1:
            self._num_black_found += 1
        if self.x_train is None:
            self.x_train = self.x_test[idx, :]
            self.y_train = []
        else:
            self.x_train = np.vstack([self.x_train, self.x_test[idx, :]])
        self.y_train.append(self.y_test.pop(idx))
        self.x_test = np.delete(self.x_test, idx, 0)

    # def run_accuracy(self, dist_calc_type, learn_type="ML"):
    #     self._init()
    #
    #     if learn_type == "DL":
    #         net = DeepLearning(self.x_train.shape[1])
    #
    #     while self._time < self._num_nodes // 5:
    #         rand = np.random.uniform(0, 1)
    #         if rand < self._eps or len(Counter(self.y_train)) <= 1:
    #             if dist_calc_type == "euclidean":
    #                 idx = DistanceCalculator.euclidean(self.x_test, self.x_train)[0]
    #             else:
    #                 idx = DistanceCalculator.one_class_learning(self.x_train, self.x_test)
    #         else:
    #             if learn_type == "DL":
    #                 idx = net.learn(self.x_train, self.y_train, self.x_test, is_al=True)
    #             else:
    #                 idx = machine_learning(self.x_train, self.y_train, self.x_test, is_al=True)
    #         self._reveal(idx)
    #
    #     # check standard ML results after revealing 20% of th data
    #     probs_ml = machine_learning(self.x_train, self.y_train, self.x_test, is_stan_ml=True)
    #     pred_ml = [1 if probs_ml[i, 1] >= 0.5 else 0 for i in range(len(probs_ml))]
    #     correct_ml = [1 if pred_ml[i] == self.y_test[i] else 0 for i in range(len(pred_ml))]
    #     accuracy_20 = sum(correct_ml) / len(correct_ml)
    #     print("mean acc using active learning: " + str(accuracy_20))
    #
    #     return self.y_train, accuracy_20


class StandardML:
    def __init__(self, features_mx, tags_vect, label=0):
        self._tags = tags_vect
        self._features_mx = features_mx
        self._smallest_class = Counter(tags_vect).most_common()[-1-label][0]
        # self._largest_class = Counter(tags_vect).most_common()[0][0]
        self._n_black = Counter(tags_vect).most_common()[-1-label][1]
        self._tags1 = [1 if y == self._smallest_class else 0 for y in self._tags]
        self._time = 0
        self._num_nodes = len(tags_vect)

    def _init(self, train_size=0.2, recall=True):
        # initialize the train objects with copies
        self.x_data = type(self._features_mx)(self._features_mx)
        if recall:
            self.y_data = list(self._tags1)
        else:
            self.y_data = list(self._tags)
        # split to train and test
        self.y_train = []
        while len(Counter(self.y_train)) <= 1:
            indices = list(range(self._num_nodes))
            split = int(np.round(self._num_nodes * train_size))
            train_idx = np.random.choice(indices, size=split, replace=False)
            test_idx = list(set(indices) - set(train_idx))
            self.x_train = np.vstack([self.x_data[i] for i in train_idx])
            self.y_train = [self.y_data[i] for i in train_idx]
            self.x_test = np.vstack([self.x_data[i] for i in test_idx])
            self.y_test = [self.y_data[i] for i in test_idx]

    def recall_per_data(self, train_size=0.2, certainty_rate=0.5):
        if train_size == 1:
            return 1, 1, 1, 1
        if train_size == 0:
            return 0, 0, 0, 0

        self._init(train_size=train_size)
        probs_ml = machine_learning(self.x_train, self.y_train, self.x_test, is_stand_ml=True)
        # revealing the true label of all nodes which classified as black
        tags = [self.y_test[i] for i in np.where(probs_ml[:, 1] >= certainty_rate)[0]]

        black_found = sum(self.y_train) + sum(tags)
        nodes_revealed = len(self.y_train) + len(tags)
        ml_recall = black_found / self._n_black
        ml_steps = nodes_revealed / self._num_nodes

        rand_recall = sum(self.y_train) / self._n_black
        rand_steps = len(self.y_train) / self._num_nodes

        return ml_recall, ml_steps, rand_recall, rand_steps

    def run_acc(self):
        self._init(train_size=0.8, recall=False)

        probs_ml = machine_learning(self.x_train, self.y_train, self.x_test, is_stand_ml=True)
        pred_ml = [np.argmax(probs_ml[i]) for i in range(len(probs_ml))]
        correct_ml = [1 if pred_ml[i] == self.y_test[i] else 0 for i in range(len(pred_ml))]
        acc_ml = sum(correct_ml)/len(correct_ml)

        # net = DeepLearning(self.x_train.shape[1], len(Counter(self.y_train)))
        # probs_dl = net.learn(self.x_train, self.y_train, self.x_test, is_stand_ml=True)
        # pred_dl = [np.argmax(probs_dl[i]) for i in range(len(probs_dl))]
        # correct_dl = [1 if pred_dl[i] == self.y_test[i] else 0 for i in range(len(pred_dl))]
        # acc_dl = sum(correct_dl) / len(correct_dl)
        acc_dl = 0

        return acc_ml,  acc_dl




