from scipy.spatial.distance import cdist
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from collections import Counter
import pyprind
import sys
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score
from random import shuffle
from scipy.stats import entropy
import xgboost as xgb
from sklearn.model_selection import train_test_split
import os
import csv
import itertools
from neural_network.nn_activator import FeedForwardNet
from neural_network.nn_models import NeuralNet


class DistanceCalculator:
    # def __init__(self, mx, typ="euclidean"):
    #     self._mx = mx
    #     self._type = typ

    @staticmethod
    def euclidean(mx1, mx2, typ="euclidean"):
        # Get euclidean distances as 2D array
        dists = cdist(mx1, mx2, typ)
        # return the most distant rows
        return np.unravel_index(dists.argmax(), (mx1.shape[0], mx2.shape[0]))

    # TODO: check more outlier detection methods, https://scikit-learn.org/stable/modules/outlier_detection.html
    @staticmethod
    def one_class_learning(x_train, x_test):
        model = svm.OneClassSVM(gamma='scale')
        model.fit(x_train)
        pred = model.predict(x_test)
        indices = np.where(pred == -1)[0]
        if len(indices) >= 1:
            random_idx = np.random.randint(0, len(indices))
            return indices[random_idx]
        else:
            return np.random.randint(0, len(x_test))


# TODO: create father class - Learning
def machine_learning(x_train, y_train, x_test, batch_size=10, clf=None, is_stand_ml=False, is_al=False, is_f1=False):
    if clf is None:
        clf = RandomForestClassifier(n_estimators=100, class_weight="balanced")
    clf.fit(np.asmatrix(x_train, dtype=np.float32), y_train)
    probs = clf.predict_proba(x_test)
    pred = clf.predict(x_test)
    if is_stand_ml:
        return pred
    if is_f1:
        # sorted_prob = [sorted(probs[i], reverse=True) for i in range(len(probs))]
        # uncertainty = [sorted_prob[i][0] - sorted_prob[i][1] for i in range(len(sorted_prob))]
        entropies = [entropy(probs[i]) for i in range(len(probs))]
        return np.argmin(entropies)
    if is_al:
        certainty = [abs(a-b) for (a, b) in probs]
        return np.argpartition(certainty, -batch_size)[-batch_size:]
    # returns the batch_size highest probability indexes
    return np.argpartition(probs[:, 1], -batch_size)[-batch_size:]


def xgb_learning(x_data, y_data, x_test, batch_size=10, is_stand_ml=False, is_al=False, is_f1=False):
    x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, train_size=0.8, random_state=0)
    clf = xgb.XGBClassifier()
    # clf.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=0)   # , early_stopping_rounds=10, eval_metric="auc")
    clf.fit(x_data, y_data, verbose=0)
    probs = clf.predict_proba(x_test)
    pred = clf.predict(x_test)
    correct_xg = [1 if pred[i] == y_data[i] else 0 for i in range(len(pred))]
    acc = sum(correct_xg) / len(correct_xg)

    param = {'booster': 'gbtree', 'max_depth': 0, 'eta': 1, 'silent': 1, 'objective': 'multi:softprob'}

    if is_stand_ml:
        return pred
    if is_f1:
        entropies = [entropy(probs[i]) for i in range(len(probs))]
        return np.argmin(entropies)

    return 0


# binary deep learning
class DeepLearning:
    def __init__(self, x_shape):
        from keras import Sequential
        from keras.callbacks import EarlyStopping
        from keras.layers import Dense, Dropout
        from keras.regularizers import l1_l2
        self.early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=50, mode='min', verbose=1, )
        self.classifier = Sequential()
        self.classifier.add(Dense(300, kernel_initializer="he_normal", activation="elu", input_dim=x_shape))
        self.classifier.add(Dropout(0.3))
        self.classifier.add(Dense(450, kernel_initializer='he_normal', activation='elu'))
        self.classifier.add(Dropout(0.3))
        self.classifier.add(Dense(100, kernel_initializer='he_normal', activation='elu'))
        self.classifier.add(Dropout(0.3))
        self.classifier.add(Dense(20, kernel_initializer='he_normal', activation='elu', kernel_regularizer=l1_l2()))
        self.classifier.add(Dense(1, kernel_initializer='uniform', activation="sigmoid",
                                  activity_regularizer=l1_l2(0.005, 0.005)))

        self.classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # TODO: implementation using keras
    def learn(self, x_train, y_train, x_test, batch_size=10, is_stand_ml=False, is_al=False):
        self.classifier.fit(x_train, y_train, validation_split=0.1, callbacks=[self.early_stopping], epochs=100,
                            batch_size=64, verbose=0)
        probs = self.classifier.predict_proba(x_test)

        if is_stand_ml:
            return probs
        if is_al:
            certainty = [abs(a-0.5) for a in probs]
            return np.argpartition(certainty, -batch_size)[-batch_size:]
        return np.argpartition(probs[:, 1], -batch_size)[-batch_size:]


# TODO: create father class - Active Learning
class ExploreExploit:
    def __init__(self, features_mx, tags_vect, label, recall):
        self._tags = tags_vect
        self._features_mx = features_mx
        classes_counter = Counter(tags_vect)
        self._smallest_class = classes_counter.most_common()[-1-label][0]
        # self._largest_class = classes_counter.most_common()[0][0]
        self._n_black = classes_counter.most_common()[-1-label][1]
        self._n_classes = len(classes_counter)
        self._tags1 = [1 if y == self._smallest_class else 0 for y in self._tags]
        self._stop_cond = np.round(recall * self._n_black)
        self._recall = recall
        self._time = 0
        self._num_black_found = 0
        self._num_nodes = len(tags_vect)
        print("there are " + str(self._n_black) + " nodes to find, which are "
              + str(self._n_black/self._num_nodes*100) + "% of the whole data")

    def _init(self):
        self._time = 0
        self._num_black_found = 0
        self._bar = pyprind.ProgBar(len(self._tags), stream=sys.stdout)
        # initialize the train objects with copies
        self.x_test = type(self._features_mx)(self._features_mx)
        self.y_test = list(self._tags1)
        self.x_train = None
        self.y_train = None
        # explore first using distance
        idx1, idx2 = DistanceCalculator.euclidean(self.x_test, self.x_test)
        self._reveal(idx2)
        self._reveal(idx1)

    def run_recall(self, dist_calc_type, learn_type="ML", batch_size=10, eps=0):
        self._init()

        time_per_recall = {round(x, 2): 0 for x in np.arange(0, self._recall + 0.01, 0.05)}
        i = 1

        if learn_type == "DL":
            net = DeepLearning(self.x_train.shape[1])

        while self._num_black_found < self._stop_cond:
            rand = np.random.uniform(0, 1)
            if rand < eps or len(Counter(self.y_train)) <= 1:
                if dist_calc_type == "euclidean":
                    idx = [DistanceCalculator.euclidean(self.x_test, self.x_train)[0]]
                else:
                    idx = [DistanceCalculator.one_class_learning(self.x_train, self.x_test)]
            else:
                if learn_type == "DL":
                    idx = net.learn(self.x_train, self.y_train, self.x_test, batch_size=min(batch_size,
                                                                                            self._num_black_found))
                else:
                    idx = machine_learning(self.x_train, self.y_train, self.x_test,
                                           batch_size=min(batch_size, self._num_black_found))
            for j in sorted(idx, reverse=True):
                self._reveal(j)
                if self._num_black_found == np.round(self._n_black * 0.05 * i):
                    time_per_recall[round(i * 0.05, 2)] = self._time / self._num_nodes
                    temp = self._num_black_found
                    i += 1
                    while np.round(self._n_black * 0.05 * i) == temp:
                        time_per_recall[round(i * 0.05, 2)] = self._time / self._num_nodes
                        i += 1
        print(" ")
        return time_per_recall, self.y_train

    def _reveal(self, idx):
        self._time += 1
        self._bar.update()
        # if self._time % 100 == 0:
        #     print(str(self._time) + " nodes were explored, time:" + str(datetime.now().time()))
        if self.y_test[idx] == 1:
            self._num_black_found += 1
        if self.x_train is None:
            self.x_train = self.x_test[idx, :]
            self.y_train = []
        else:
            self.x_train = np.vstack([self.x_train, self.x_test[idx, :]])
        self.y_train.append(self.y_test.pop(idx))
        self.x_test = np.delete(self.x_test, idx, 0)


class ExploreExploitF1:
    def __init__(self, features_mx, tags_vect, budget):
        self._tags = tags_vect
        self._features_mx = features_mx
        self._n_classes = len(Counter(tags_vect))
        self._stop_cond = budget * self._n_classes
        self._num_nodes = len(tags_vect)

    def _init(self, n_nodes=4):
        # initialize the train objects with copies
        self.x_data = type(self._features_mx)(self._features_mx)
        self.y_data = list(self._tags)

        self.y_pool = [1]
        while min(Counter(self.y_pool).values()) < 5:
            indices = list(range(self._num_nodes))
            split = min(1000, self._num_nodes // 2)
            test_idx = np.random.choice(indices, size=split, replace=False)
            pool_idx = list(set(indices) - set(test_idx))
            self.x_pool = np.vstack([self.x_data[i] for i in pool_idx])
            self.y_pool = [self.y_data[i] for i in pool_idx]
            self.x_test = np.vstack([self.x_data[i] for i in test_idx])
            self.y_test = [self.y_data[i] for i in test_idx]

        self.x_train = None
        self.y_train = []
        c = {x: 0 for x in self._tags}
        indices = list(range(len(self.x_pool)))
        shuffle(indices)
        idx = []
        i = 0
        while min(c.values()) < 4:
            if c[self.y_pool[indices[i]]] < 4:
                idx.append(indices[i])
                c[self.y_pool[indices[i]]] += 1
            i += 1

        idx.sort(reverse=True)
        for i in idx:
            self._reveal(i)

    def run(self, eps=0):
        self._init(n_nodes=4)

        while len(self.x_train) <= self._stop_cond:
            rand = np.random.uniform(0, 1)
            if rand < eps:
                idx = np.random.choice(range(len(self.x_pool)))
            else:
                # idx = machine_learning(self.x_train, self.y_train, self.x_pool, is_f1=True)
                idx = xgb_learning(self.x_train, self.y_train, self.x_pool, is_f1=True)
            self._reveal(idx)

        y_pred = machine_learning(self.x_train, self.y_train, self.x_train, is_stand_ml=True)

        train_macrof1 = f1_score(self.y_train, y_pred, average='macro')
        train_microf1 = f1_score(self.y_train, y_pred, average='micro')

        y_pred = machine_learning(self.x_train, self.y_train, self.x_test, is_stand_ml=True)

        macrof1 = f1_score(self.y_test, y_pred, average='macro')
        microf1 = f1_score(self.y_test, y_pred, average='micro')

        return macrof1, microf1

    def _reveal(self, idx):
        if self.x_train is None:
            self.x_train = self.x_pool[idx, :]
        else:
            self.x_train = np.vstack([self.x_train, self.x_pool[idx, :]])
        self.y_train.append(self.y_pool.pop(idx))
        self.x_pool = np.delete(self.x_pool, idx, 0)


class StandardML:
    def __init__(self, features_mx, tags_vect, label=0):
        self._tags = tags_vect
        self._features_mx = features_mx
        counter_class = Counter(tags_vect)
        self._n_class = len(counter_class)
        self._smallest_class = counter_class.most_common()[-1-label][0]
        # self._largest_class = counter_class.most_common()[0][0]
        self._n_black = counter_class.most_common()[-1-label][1]
        self._tags1 = [1 if y == self._smallest_class else 0 for y in self._tags]
        self._time = 0
        self._num_nodes = len(tags_vect)

    def _init(self, train_size=0.2, recall=True):
        # initialize the train objects with copies
        self.x_data = type(self._features_mx)(self._features_mx)
        if recall:
            self.y_data = list(self._tags1)
        else:
            self.y_data = list(self._tags)
        # split to train and test
        self.y_train = []
        while len(Counter(self.y_train)) <= 1:
            indices = list(range(self._num_nodes))
            split = int(np.round(self._num_nodes * train_size))
            train_idx = np.random.choice(indices, size=split, replace=False)
            test_idx = list(set(indices) - set(train_idx))
            self.x_train = np.vstack([self.x_data[i] for i in train_idx])
            self.y_train = [self.y_data[i] for i in train_idx]
            self.x_test = np.vstack([self.x_data[i] for i in test_idx])
            self.y_test = [self.y_data[i] for i in test_idx]

    def recall_per_data(self, train_size=0.2, certainty_rate=0.5):
        if train_size == 1:
            return 1, 1, 1, 1
        if train_size == 0:
            return 0, 0, 0, 0

        self._init(train_size=train_size)
        probs_ml, classes = machine_learning(self.x_train, self.y_train, self.x_test, is_stand_ml=True)
        # revealing the true label of all nodes which classified as black
        tags = [self.y_test[i] for i in np.where(probs_ml[:, 1] >= certainty_rate)[0]]

        black_found = sum(self.y_train) + sum(tags)
        nodes_revealed = len(self.y_train) + len(tags)
        ml_recall = black_found / self._n_black
        ml_steps = nodes_revealed / self._num_nodes

        rand_recall = sum(self.y_train) / self._n_black
        rand_steps = len(self.y_train) / self._num_nodes

        return ml_recall, ml_steps, rand_recall, rand_steps

    def run_acc(self, train_size=0.8, print_train=False):
        self._init(train_size=train_size, recall=False)
        # XGBoost
        pred_xg = machine_learning(self.x_train, self.y_train, self.x_test, is_stand_ml=True)
        acc_xg = accuracy_score(pred_xg, self.y_test)

        # Random Forest
        pred_rf = machine_learning(self.x_train, self.y_train, self.x_test, is_stand_ml=True)
        acc_rf = accuracy_score(pred_rf, self.y_test)

        # Neural Network
        net = FeedForwardNet(NeuralNet(classes=self._n_class), gpu=True)
        net.set_data(self.x_train, self.y_train)
        net.train(total_epoch=100, stop_loss=True)
        pred_dl = net.predict(self.x_test)
        acc_dl = accuracy_score(pred_dl, self.y_test)

        if print_train:
            pred_xg = xgb_learning(self.x_train, self.y_train, self.x_train, is_stand_ml=True)
            train_acc_xg = accuracy_score(pred_xg, self.y_train)

            pred_rf = machine_learning(self.x_train, self.y_train, self.x_train, is_stand_ml=True)
            train_acc_rf = accuracy_score(pred_rf, self.y_train)

            pred_dl = net.predict(self.x_train)
            train_acc_dl = accuracy_score(pred_dl, self.y_train)

            print("train accuracy is:  XGB - " + str(train_acc_xg) +
                  "   RF - " + str(train_acc_rf) + "   NN - " + str(train_acc_dl))

        return acc_xg, acc_rf, acc_dl


def grid_learn_xgb(data_name, x_data, labels, epochs=100):
    classes = {}
    c = Counter(labels)
    for i, tag in enumerate(c):
        classes[tag] = i
    new_labels = [classes[i] for i in labels]

    if not os.path.exists(os.path.join(os.getcwd(), 'parameter_check')):
        os.mkdir('parameter_check')
    if not os.path.exists(os.path.join(os.getcwd(), 'parameter_check', data_name)):
        os.mkdir(os.path.join('parameter_check', data_name))
    # train percentage
    for train_p in [70]:
        f = open(os.path.join(os.getcwd(), 'parameter_check', data_name, "results_train_p" + str(train_p) +
                              "_dart" + ".csv"), 'w')
        w = csv.writer(f)
        w.writerow(['max_depth', 'lambda', 'eta', 'min child weight', 'subsample', 'ntree_limit', 'sample type',
                    'normalize type', 'rate drop', 'train_microF1', 'train_macroF1', 'train acc', 'test_microF1',
                    'test_macroF1', 'test acc'])
        for max_depth, l, eta, min_child_weight, subsample, ntree_limit, sample_type, normalize_type, rate_drop in \
                itertools.product(range(3, 12, 4), range(1, 20, 6), np.logspace(-3, -0.5, 5), range(1, 14, 6),
                                  range(6, 11, 2), range(1, 122, 40), ['uniform', 'weighted'], ['tree', 'forest'],
                                  range(2, 9, 3)):
            acc_train = []
            acc_test = []
            macrof1_test = []
            microf1_test = []
            macrof1_train = []
            microf1_train = []
            for num_splits in range(1, epochs+1):
                x_train, x_test, y_train, y_test = train_test_split(x_data, new_labels, test_size=1 - float(train_p)/100)
                x_train, x_eval, y_train, y_eval = train_test_split(x_train, y_train, test_size=0.1)
                dtrain = xgb.DMatrix(x_train, label=y_train, silent=True)
                dtest = xgb.DMatrix(x_test, label=y_test, silent=True)
                deval = xgb.DMatrix(x_eval, label=y_eval, silent=True)
                params = {'silent': True, 'booster': 'dart', 'tree_method': 'auto', 'max_depth': max_depth,
                          'lambda': l / 10, 'eta': eta, 'min_child_weight': min_child_weight,
                          'subsample': subsample / 10, 'sample_type': sample_type, 'normalize_type': normalize_type,
                          'rate_drop': rate_drop / 10, 'objective': 'multi:softprob', 'num_class': len(c)}
                clf_xgb = xgb.train(params, dtrain=dtrain, evals=[(dtrain, 'train'), (deval, 'eval')],
                                    early_stopping_rounds=10, verbose_eval=False)
                y_score_test = clf_xgb.predict(dtest, ntree_limit=ntree_limit)
                y_pred_test = [np.argmax(i) for i in y_score_test]
                y_score_train = clf_xgb.predict(dtrain, ntree_limit=ntree_limit)
                y_pred_train = [np.argmax(i) for i in y_score_train]
                # ROC AUC has a problem with only one class
                try:
                    macf1 = f1_score(y_test, y_pred_test, average='macro')
                    micf1 = f1_score(y_test, y_pred_test, average='micro')
                    test_acc = accuracy_score(y_test, y_pred_test)
                    # r1 = roc_auc_score(y_test, y_score_test)
                except ValueError:
                    continue
                macrof1_test.append(macf1)
                microf1_test.append(micf1)
                acc_test.append(test_acc)

                try:
                    macf1 = f1_score(y_train, y_pred_train, average='macro')
                    micf1 = f1_score(y_train, y_pred_train, average='micro')
                    train_acc = accuracy_score(y_train, y_pred_train)
                    # r2 = roc_auc_score(y_train, y_score_train)
                except ValueError:
                    continue
                macrof1_train.append(macf1)
                microf1_train.append(micf1)
                acc_train.append(train_acc)
            w.writerow([str(max_depth), str(l / 10), str(eta), str(min_child_weight),
                        str(subsample / 10), str(ntree_limit), str(sample_type), str(normalize_type),
                        str(rate_drop / 10), str(np.mean(microf1_train)), str(np.mean(macrof1_train)),
                        str(np.mean(acc_train)), str(np.mean(microf1_test)), str(np.mean(macrof1_test)),
                        str(np.mean(acc_test))])
    return None


def grid_learn_nn(data_name, x_data, labels, epochs=100):
    num_classes = len(Counter(labels))

    if not os.path.exists(os.path.join(os.getcwd(), 'parameter_check')):
        os.mkdir('parameter_check')
    if not os.path.exists(os.path.join(os.getcwd(), 'parameter_check', data_name)):
        os.mkdir(os.path.join('parameter_check', data_name))
    # train percentage
    for train_p in [80]:
        f = open(os.path.join(os.getcwd(), 'parameter_check', data_name, "results_train_p" + str(train_p) + "_nn" + ".csv"), 'w')
        w = csv.writer(f)
        w.writerow(['batch size', 'layers dim', 'lr', 'drop out', 'min l2 penalty weight', 'activation func',
                    'macro f1 test', 'micro f1 test', 'acc test', 'loss test',
                    'macro f1 train', 'micro f1 train', 'acc train', 'loss train'])
        for batch_size, layers_dim, lr, drop_out, l2_penalty, activation_func in \
                itertools.product([32, 64, 128], [(225, 140, 70), (225, 350, 150, 60), (225, 600, 320, 160, 70)],
                                  [0.0001, 0.0005, 0.001, 0.0015, 0.003], range(1, 11, 2),  np.logspace(-3, -0.5, 5),
                                  ["relu", "elu"]):
            acc_train = []
            acc_test = []
            loss_train = []
            loss_test = []
            macrof1_test = []
            microf1_test = []
            macrof1_train = []
            microf1_train = []
            for num_splits in range(1, epochs+1):
                x_train, x_test, y_train, y_test = train_test_split(x_data, labels, test_size=1 - float(train_p) / 100)

                net = FeedForwardNet(NeuralNet(num_classes, batch_size=batch_size, layers_dim=layers_dim,
                                               drop_out=drop_out / 10, l2_penalty=l2_penalty,
                                               activation_func=activation_func, lr=lr), gpu=True)
                net.set_data(x_train, y_train)
                net.train(total_epoch=100, stop_loss=True)
                train_loss = net.test(train=True)
                test_loss = net.test()
                y_pred_train = net.predict(x_train)
                y_pred_test = net.predict(x_test)

                # scores for test
                macf1 = f1_score(y_test, y_pred_test, average='macro')
                micf1 = f1_score(y_test, y_pred_test, average='micro')
                test_acc = accuracy_score(y_test, y_pred_test)
                macrof1_test.append(macf1)
                microf1_test.append(micf1)
                acc_test.append(test_acc)
                loss_test.append(test_loss)

                # scores for train
                macf1 = f1_score(y_train, y_pred_train, average='macro')
                micf1 = f1_score(y_train, y_pred_train, average='micro')
                train_acc = accuracy_score(y_train, y_pred_train)
                macrof1_train.append(macf1)
                microf1_train.append(micf1)
                acc_train.append(train_acc)
                loss_train.append(train_loss)

            w.writerow([str(batch_size), str(layers_dim), str(lr), str(drop_out), str(l2_penalty),
                        str(activation_func), str(np.mean(macrof1_test)), str(np.mean(microf1_test)),
                        str(np.mean(acc_test)), str(np.mean(loss_test)), str(np.mean(macrof1_train)),
                        str(np.mean(microf1_train)), str(np.mean(acc_train)), str(np.mean(loss_train))])
    return None
