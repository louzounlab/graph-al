from scipy.spatial.distance import cdist
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from collections import Counter
from datetime import datetime
import pyprind
import sys
import time

from sklearn.model_selection import train_test_split


class DistanceCalculator:
    # def __init__(self, mx, typ="euclidean"):
    #     self._mx = mx
    #     self._type = typ

    def euclidean(self, mx1, mx2, typ="euclidean"):
        # Get euclidean distances as 2D array
        dists = cdist(mx1, mx2, typ)
        # return the most distant rows
        return np.unravel_index(dists.argmax(), (mx1.shape[0], mx2.shape[0]))

    # TODO:implement using one class svm- http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html
    def one_class_learning(self, x_train, x_test):
        model = svm.OneClassSVM(gamma='scale')
        model.fit(x_train)
        pred = model.predict(x_test)
        indices = np.where(pred == -1)[0]
        if len(indices) >=1:
            random_idx = np.random.randint(0, len(indices))
            return indices[random_idx]
        else:
            return np.random.randint(0, len(x_test))


class Learning:
    def machine_learning(self, x_train, y_train, x_test, clf=None, is_stan_ml=False, is_al=False):
        if clf is None:
            clf = RandomForestClassifier(n_estimators=200, class_weight="balanced")
        clf.fit(np.asmatrix(x_train, dtype=np.float32), y_train)
        probs = clf.predict_proba(x_test)
        if is_stan_ml:
            return probs
        if is_al:
            certainty = [abs(a-b) for (a, b) in probs]
            return certainty.index(min(certainty))
        return probs.argmax(0)[1]

    # TODO: implementation using keras
    def deep_learning(self, x_train, y_train, x_test, is_stan_ml=False, is_al=False):
        # pass
        from keras import Sequential
        from keras.callbacks import EarlyStopping
        from keras.layers import Dense, Dropout
        from keras.regularizers import l1_l2
        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=50, mode='min', verbose=1, )
        self.classifier = Sequential()
        self.classifier.add(Dense(300, kernel_initializer="he_normal", activation="elu", input_dim=x_train.shape[1]))
        self.classifier.add(Dropout(0.3))
        self.classifier.add(Dense(450, kernel_initializer='he_normal', activation='elu'))
        self.classifier.add(Dropout(0.3))
        self.classifier.add(Dense(100, kernel_initializer='he_normal', activation='elu'))
        self.classifier.add(Dropout(0.3))
        self.classifier.add(Dense(20, kernel_initializer='he_normal', activation='elu', kernel_regularizer=l1_l2()))
        self.classifier.add(Dense(1, kernel_initializer='uniform', activation="sigmoid",
                                  activity_regularizer=l1_l2(0.005, 0.005)))

        self.classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        self.classifier.fit(x_train, y_train, validation_split=0.1, callbacks=[early_stopping], epochs=100,
                            batch_size=64, verbose=0)

        probs = self.classifier.predict_proba(x_test)

        if is_stan_ml:
            return probs
        if is_al:
            certainty = [abs(a-0.5) for a in probs]
            return certainty.index(min(certainty))
        return probs.argmax()


class ExploreExploit:
    def __init__(self, tags_vect, features_mx, recall, eps):
        self._tags = tags_vect
        self._features_mx = features_mx
        self._smallest_class = Counter(tags_vect).most_common()[-1][0]
        # self._largest_class = Counter(tags_vect).most_common()[0][0]
        self._n_black = Counter(tags_vect).most_common()[-1][1]
        self._tags1 = [1 if y == self._smallest_class else 0 for y in self._tags]
        self._stop_cond = np.round(recall * self._n_black)
        self._eps = eps
        self._time = 0
        self._num_black_found = 0
        self._num_nodes = len(tags_vect)

    def _init(self):
        # initialize the train objects with copies
        self.x_test = type(self._features_mx)(self._features_mx)
        self.y_test = list(self._tags1)
        self.x_train = None
        self.y_train = None
        self.bar = pyprind.ProgBar(len(self._tags), stream=sys.stdout)
        # explore first using distance
        idx1, idx2 = DistanceCalculator().euclidean(self.x_test, self.x_test)
        self._reveal(idx1)
        self._reveal(idx2-1)    # we deleted idx1 from the test, therefore the original index idx2 is now idx2-1
                                # notice idx1 is smaller then idx2

    def run(self, dist_calc_type, learn_type="ML"):
        self._num_black_found = 0
        self._init()
        accuracy_0_40 = 0
        count_0_40 = 0
        accuracy_20 = 0

        while self._num_black_found < self._stop_cond:
            rand = np.random.uniform(0, 1)
            if rand < self._eps or len(Counter(self.y_train)) <= 1:
                if dist_calc_type == "euclidean":
                    idx = DistanceCalculator().euclidean(self.x_test, self.x_train)[0]
                else:
                    idx = DistanceCalculator().one_class_learning(self.x_train, self.x_test)
            else:
                if learn_type == "ML":
                    idx = Learning().machine_learning(self.x_train, self.y_train, self.x_test)
                elif learn_type == "DL":
                    idx = Learning().deep_learning(self.x_train, self.y_train, self.x_test)
                if self._time <= self._num_nodes * 0.4:
                    count_0_40 += 1
                    if self.y_test[idx] == 1:
                        accuracy_0_40 += 1
            # check standard ML results after revealing 20% of th data
            if self._time == self._num_nodes // 5:
                probs_ml = Learning().machine_learning(self.x_train, self.y_train, self.x_test, is_stan_ml=True)
                pred_ml = [1 if probs_ml[i, 1] >= 0.5 else 0 for i in range(len(probs_ml))]
                correct_ml = [1 if pred_ml[i] == self.y_test[i] else 0 for i in range(len(pred_ml))]
                accuracy_20 = sum(correct_ml) / len(correct_ml)

            self._reveal(idx)

        accuracy_0_40 /= count_0_40
        print("\n mean acc till 40% is: " + str(accuracy_0_40) + " and from 20% using stand ml: " + str(accuracy_20))

        return self._time/self._num_nodes, self.y_train, accuracy_0_40, accuracy_20

    def _reveal(self, idx):
        self._time += 1
        self.bar.update()
        # if self._time % 100 == 0:
        #     print(str(self._time) + " nodes were explored, time:" + str(datetime.now().time()))
        if self.y_test[idx] == 1:
            self._num_black_found += 1
        if self.x_train is None:
            self.x_train = self.x_test[idx, :]
            self.y_train = []
        else:
            self.x_train = np.vstack([self.x_train, self.x_test[idx, :]])
        self.y_train.append(self.y_test.pop(idx))
        self.x_test = np.delete(self.x_test, idx, 0)

    def run_opposite_active(self, dist_calc_type, learn_type="ML"):
        self._init()

        while self._time < self._num_nodes // 5:
            rand = np.random.uniform(0, 1)
            if rand < self._eps or len(Counter(self.y_train)) <= 1:
                if dist_calc_type == "euclidean":
                    idx = DistanceCalculator().euclidean(self.x_test, self.x_train)[0]
                else:
                    idx = DistanceCalculator().one_class_learning(self.x_train, self.x_test)
            else:
                if learn_type == "ML":
                    idx = Learning().machine_learning(self.x_train, self.y_train, self.x_test, is_al=True)
                elif learn_type == "DL":
                    idx = Learning().deep_learning(self.x_train, self.y_train, self.x_test, is_al=True)
            self._reveal(idx)

        # check standard ML results after revealing 20% of th data
        probs_ml = Learning().machine_learning(self.x_train, self.y_train, self.x_test, is_stan_ml=True)
        pred_ml = [1 if probs_ml[i, 1] >= 0.5 else 0 for i in range(len(probs_ml))]
        correct_ml = [1 if pred_ml[i] == self.y_test[i] else 0 for i in range(len(pred_ml))]
        accuracy_20 = sum(correct_ml) / len(correct_ml)
        print("mean acc using active learning: " + str(accuracy_20))

        return self.y_train, accuracy_20


class StandardML:
    def __init__(self, tags_vect, features_mx):
        self._tags = tags_vect
        self._features_mx = features_mx
        self._smallest_class = Counter(tags_vect).most_common()[-1][0]
        # self._largest_class = Counter(tags_vect).most_common()[0][0]
        self._n_black = Counter(tags_vect).most_common()[-1][1]
        self._tags1 = [1 if y == self._smallest_class else 0 for y in self._tags]
        self._time = 0
        self._num_nodes = len(tags_vect)

    def _init(self):
        # initialize the train objects with copies
        self.x_data = type(self._features_mx)(self._features_mx)
        self.y_data = list(self._tags1)

        # split to train and test
        indices = list(range(self._num_nodes))
        split = self._num_nodes // 5
        # idx = np.random.choice(indices, size=split, replace=False)
        # train_idx = np.random.choice(idx, size=split // 2, replace=False)
        # test_idx = list(set(idx) - set(train_idx))
        train_idx = np.random.choice(indices, size=split, replace=False)
        test_idx = list(set(indices) - set(train_idx))
        self.x_train = np.vstack([self.x_data[i] for i in train_idx])
        self.y_train = [self.y_data[i] for i in train_idx]
        self.x_test = np.vstack([self.x_data[i] for i in test_idx])
        self.y_test = [self.y_data[i] for i in test_idx]

        self.bar = pyprind.ProgBar(len(self._tags), stream=sys.stdout)

    def run(self):
        self._init()

        probs_ml = Learning().machine_learning(self.x_train, self.y_train, self.x_test, is_stan_ml=True)
        pred_ml = [1 if probs_ml[i, 1] >= 0.5 else 0 for i in range(len(probs_ml))]
        correct_ml = [1 if pred_ml[i] == self.y_test[i] else 0 for i in range(len(pred_ml))]
        acc_ml = sum(correct_ml)/len(correct_ml)

        probs_dl = Learning().deep_learning(self.x_train, self.y_train, self.x_test, is_stan_ml=True)
        pred_dl = [1 if probs_dl[i] >= 0.5 else 0 for i in range(len(probs_dl))]
        correct_dl = [1 if pred_dl[i] == self.y_test[i] else 0 for i in range(len(pred_dl))]
        acc_dl = sum(correct_dl) / len(correct_dl)

        return acc_ml, acc_dl

